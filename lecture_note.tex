\documentclass{report}
\usepackage{authblk}
\usepackage{mathptmx}
\usepackage{url,latexsym,amsmath,amsthm,xspace,rotating,multirow,multicol,xspace,amssymb,paralist}
\usepackage{euscript}
\usepackage{fancybox,xcolor}
\usepackage{longtable}
\usepackage{paralist}
\usepackage[normalem]{ulem}
\usepackage[pdftex]{hyperref}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{cancel}
\usepackage{mathtools}

\usepackage{url}
\usepackage{latexsym}

\usepackage{times}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{tabularx}
\usepackage{multicol}
\usepackage{multirow}
%\usepackage{hyperref}
\usepackage{url}
%\usepackage{natbib}
\usepackage{wrapfig}
\usepackage{comment}
\usepackage{listings}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{fancyvrb}
\usepackage{booktabs}
\usepackage{color}
\usepackage[normalem]{ulem}

\newcommand{\obs}{\text{obs}}
\newcommand{\mis}{\text{mis}}

\newcommand{\qt}[1]{\left<#1\right>}
\newcommand{\ql}[1]{\left[#1\right]}
\newcommand{\hess}{\mathbf{H}}
\newcommand{\jacob}{\mathbf{J}}
\newcommand{\hl}{HL}
\newcommand{\cost}{\mathcal{L}}
\newcommand{\lout}{\mathbf{r}}
\newcommand{\louti}{r}
\newcommand{\outi}{y}
\newcommand{\out}{\mathbf{y}}
\newcommand{\gauss}{\mathbf{G_N}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\softmax}{\text{softmax}}
\newcommand{\targ}{\mathbf{t}}
\newcommand{\metric}{\mathbf{G}}
\newcommand{\sample}{\mathbf{z}}
\newcommand{\f}{\text{f}}
%\newcommand{\log}{\text{log}}

\newcommand{\bmx}[0]{\begin{bmatrix}}
\newcommand{\emx}[0]{\end{bmatrix}}
\newcommand{\qexp}[1]{\left<#1\right>}
\newcommand{\vect}[1]{\mathbf{#1}}
\newcommand{\vects}[1]{\boldsymbol{#1}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\var}[0]{\operatorname{Var}}
\newcommand{\std}[0]{\operatorname{std}}
\newcommand{\cov}[0]{\operatorname{Cov}}
\newcommand{\diag}[0]{\operatorname{diag}}
\newcommand{\matrs}[1]{\boldsymbol{#1}}
\newcommand{\va}[0]{\vect{a}}
\newcommand{\vb}[0]{\vect{b}}
\newcommand{\vc}[0]{\vect{c}}
\newcommand{\ve}[0]{\vect{e}}

\newcommand{\vh}[0]{\vect{h}}
\newcommand{\vv}[0]{\vect{v}}
\newcommand{\vx}[0]{\vect{x}}
\newcommand{\vp}[0]{\vect{p}}
\newcommand{\vz}[0]{\vect{z}}
\newcommand{\vw}[0]{\vect{w}}
\newcommand{\vs}[0]{\vect{s}}
\newcommand{\vf}[0]{\vect{f}}
\newcommand{\vi}[0]{\vect{i}}
\newcommand{\vo}[0]{\vect{o}}
\newcommand{\vd}[0]{\vect{d}}
\newcommand{\vy}[0]{\vect{y}}
\newcommand{\vg}[0]{\vect{g}}
\newcommand{\vm}[0]{\vect{m}}
\newcommand{\vu}[0]{\vect{u}}
\newcommand{\vL}[0]{\vect{L}}
\newcommand{\vr}[0]{\vect{r}}
\newcommand{\vone}[0]{\vect{1}}

\newcommand{\mW}[0]{\matr{W}}
\newcommand{\mE}[0]{\matr{E}}
\newcommand{\mG}[0]{\matr{G}}
\newcommand{\mX}[0]{\matr{X}}
\newcommand{\mY}[0]{\matr{Y}}
\newcommand{\mQ}[0]{\matr{Q}}
\newcommand{\mU}[0]{\matr{U}}
\newcommand{\mF}[0]{\matr{F}}
\newcommand{\mV}[0]{\matr{V}}
\newcommand{\mA}{\matr{A}}
\newcommand{\mC}{\matr{C}}
\newcommand{\mD}{\matr{D}}
\newcommand{\mL}[0]{\matr{L}}
\newcommand{\mR}[0]{\matr{R}}
\newcommand{\mS}{\matr{S}}
\newcommand{\mI}{\matr{I}}
\newcommand{\td}[0]{\text{d}}
\newcommand{\TT}[0]{\vects{\theta}}
\newcommand{\vsig}[0]{\vects{\sigma}}
\newcommand{\valpha}[0]{\vects{\alpha}}
\newcommand{\vmu}[0]{\vects{\mu}}
\newcommand{\vzero}[0]{\vect{0}}
\newcommand{\tf}[0]{\text{m}}
\newcommand{\tdf}[0]{\text{dm}}
\newcommand{\grad}[0]{\nabla}
\newcommand{\alert}[1]{\textcolor{red}{#1}}
\newcommand{\N}[0]{\mathcal{N}}
\newcommand{\YY}[0]{\mathcal{Y}}
\newcommand{\BB}[0]{\mathcal{B}}
\newcommand{\LL}[0]{\mathcal{L}}
\newcommand{\HH}[0]{\mathcal{H}}
\newcommand{\RR}[0]{\mathbb{R}}
\newcommand{\MM}[0]{\mathcal{M}}
\newcommand{\OO}[0]{\mathbb{O}}
\newcommand{\II}[0]{\mathbb{I}}
\newcommand{\Scal}[0]{\mathcal{S}}
\newcommand{\sigmoid}{\sigma}
\newcommand{\sign}{\text{sign}}
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\enabla}[0]{\ensuremath{%
    \overset{\raisebox{-0.3ex}[0.5ex][0ex]{%
    \ensuremath{\scriptscriptstyle e}}}{\nabla}}}
\newcommand{\enhnabla}[0]{\nabla_{\hspace{-0.5mm}e}\,}
\newcommand{\eos}[0]{\ensuremath{\left< \text{eos}\right>}}


\newcommand{\todo}[1]{{\Large\textcolor{red}{#1}}}
\newcommand{\done}[1]{{\Large\textcolor{green}{#1}}}
\newcommand{\dd}[1]{\ensuremath{\mbox{d}#1}}

\DeclareMathOperator*{\argmax}{\arg \max}
\DeclareMathOperator*{\argmin}{\arg \min}
\newcommand{\newln}{\\&\quad\quad{}}

\newcommand{\BP}{\text{BP}}
\newcommand{\PPL}{\text{PPL}}
\newcommand{\PL}{\text{PL}}
\newcommand{\MatSum}{\text{MatSum}}
\newcommand{\MatMul}{\text{MatMul}}
\newcommand{\KL}{\text{KL}}
\newcommand{\data}{\text{data}}
\newcommand{\rect}{\text{rect}}
\newcommand{\maxout}{\text{maxout}}
\newcommand{\train}{\text{train}}
\newcommand{\val}{\text{val}}
\newcommand{\init}{\text{init}}
\newcommand{\fenc}{\text{fenc}}
\newcommand{\renc}{\text{renc}}
\newcommand{\enc}{\text{enc}}
\newcommand{\dec}{\text{dec}}
\newcommand{\test}{\text{test}}
\newcommand{\Ax}{\mathcal{A}_x}
\newcommand{\Ay}{\mathcal{A}_y}
\newcommand{\ola}{\overleftarrow}
\newcommand{\ora}{\overrightarrow}
\newcommand{\ov}{\overline}
\newcommand{\ts}{\rule{0pt}{2.6ex}}       % Top strut
\newcommand{\ms}{\rule{0pt}{0ex}}         % Middle strut
\newcommand{\bs}{\rule[-1.2ex]{0pt}{0pt}} % Bottom strut
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}


%\usepackage{bibentry}
%\nobibliography*

\begin{document}

\title{Introduction to Machine Learning}
\author{Kyunghyun Cho}
\affil{
    Courant Institute of Mathematical Sciences and \\
    Center for Data Science,\\
    New York University 
}

\maketitle
\pagenumbering{arabic}

\abstract{
    This is a lecture note for the course CSCI-UA.0473-001 (Intro to Machine
    Learning) at the Department of Computer Science, Courant Institute of
    Mathematical Sciences at New York University. The content of the lecture
    note was selected to fit a single 12-week-long course (3 hours a week) and
    to mainly serve undergraduate students majoring in computer science.  Many
    existing materials in machine learning therefore had to be omitted. 
    
    For a more complete coverage of machine learning (with math!), the following
    text books are recommended in addition to this lecture note:
    \begin{itemize}
        \item ``Pattern Recognition and Machine Learning'' by Chris Bishop \cite{bishop2006pattern}
        \item ``Machine Learning: a probabilistic perspective'' by Kevin Murphy \cite{murphy2012machine}
        \item ``A Course in Machine Learning'' by Hal Daum\'e\footnote{
                Available at \url{http://ciml.info/}.
            }
    \end{itemize}

    For practical exercises, Python scripts based on numpy and scipy are
    available at
    \url{https://github.com/nyu-dl/Intro_to_ML_Lecture_Note/tree/master/notebook}.
    They are under heavy development and subject to frequent changes over the
    course. I recommend you to check back frequently. Again, these are not
    exhaustive, and for a more complete coverage on machine learning practice, I
    recommend the following book:
    \begin{itemize}
        \item ``Introduction to Machine Learning with Python'' by Andreas
            M\"uller  and Sarah Guido
    \end{itemize}

    Note that those sections marked with $\star$ are optional and will only be
    covered should time permits. 
}

\chapter*{Notations}

Throughout this lecture note, I will use the following notational conventions:
\begin{itemize}
    \item A bold-faced lower-case alphabet is used for a vector: $\vx$
    \item A bold-faced upper-case alphabet is used for a matrix: $\mW$
    \item A lower-case alphabet is often used for a scalar: $x$, $\eta$
    \item A lower-case alphabet is also used for denoting a function
    \item 
\end{itemize}


\chapter{Classification}
\label{sec:classification}

\section{Supervised Learning}

In supervised learning, our goal is to build or find a machine $M$ that takes as
input a multi-dimensional vector $\vx \in \mathbb{R}^d$ and outputs a response
vector $\vy \in \mathbb{R}^{d'}$.  That is,
\begin{align*}
    M: \mathbb{R}^d \to \mathbb{R}^{d'}.
\end{align*}
Of course this cannot be done out of blue, and we first assume that there exists
a reference design $M^*$ of such a machine.  We then refine our goal as to build
or find a machine $M$ that imitates the reference machine $M^*$ as closely as
possible. In other words, we want to make sure that for any given $\vx$, the
outputs of $M$ and $M^*$ coincide, i.e.,
\begin{align}
    \label{eq:classification0}
    M(\vx) = M^*(\vx),\text{ for all } \vx \in \mathbb{R}^d.
\end{align}
This is still not enough for us to find $M$, because there are infinitely many
possible $M$'s through which we must search. We must hence decide on our {\it
hypothesis set} $H$ of potential machines. This is an important decision, as it
directly influences the difficulty in finding such a machine. When your
hypothesis set is not constructed well, there may not be a machine that
satisfies the criterion above. 

We can state the same thing in a slightly different way. First, let us assume a
function $D$ that takes as input the output of $M^*$, a machine $M$ and an input
vector $\vx$, and returns how much they differ from each other, i.e.,
\begin{align*}
    D: \mathbb{R}^{d'} \times H \times \mathbb{R}^{d'} \to \mathbb{R}_+,
\end{align*}
where $\mathbb{R}_+$ is a set of non-negative real numbers. As usual in our
everyday life, the smaller the output of $D$ the more similar the outputs of $M$
and $M^*$. An example of such a function would be
\begin{align*}
    D(y, M, \vx) = \left\{
        \begin{array}{l l}
            0, & \text{if }  y = M(\vx) \\
            1, & \text{otherwise}
        \end{array}
        \right..
\end{align*}

It is certainly possible to tailor this distance function, or a {\it
per-example cost} function, for a specific target task. For instance, consider
an intrusion detection system $M$ which takes as input a video frame of a store
front and returns a binary indicator, instead of a real number, whether there is
a thief in front of the store (0: no and 1: yes). When there is no thief
($M^*(\vx)=0$), it does not cost you anything when $M$ agrees with $M^*$, but
you must pay \$10 for security dispatch if $M$ predicted $1$. When there is a
thief in front of your store ($M^*(\vx)=1$), you will lose \$100 if the alarm
fails to detect the thief ($M(\vx)=0$) but will not lose any if the alarm went
off. In this case, we may define the per-example cost function as
\begin{align*}
    D(y, M, \vx) = \left\{
        \begin{array}{l l}
            0, & \text{if }  y=M(\vx) \\
            -10, & \text{if }  y=0\text{ and }M(\vx)=1 \\
            -100, & \text{if }  y=1\text{ and }M(\vx)=0 
        \end{array}
        \right..
\end{align*}
Note that this distance is asymmetric.

Given a distance function $D$, we can now state the supervised learning
problem as finding a machine $M$, with in a given hypothesis set $H$, that
minimizes its distance from the reference machine $M^*$ for any given input.
That is,
\begin{align}
    \label{eq:classification1}
    \argmin_{M \in H} \int_{\mathbb{R}^d} D(M^*(\vx), M, \vx) \text{d}\vx.
\end{align}

You may have noticed that these two conditions in
Eqs.~\eqref{eq:classification0}--\eqref{eq:classification1} are not equivalent.
If a machine $M$ satisfies the first condition, the second conditions is
naturally satisfied. The other way around however does not necessarily hold.
Even then, we prefer the second condition as our ultimate goal to satisfy in
machine learning. This is because we often cannot guarantee that $M^*$ is
included in the hypothesis set $H$. The first condition simple becomes
impossible to satisfy when $M^* \notin H$, but the second condition gets us a
machine $M$ that is {\it close} enough to the reference machine $M^*$. We prefer
to have a suboptimal solution rather than having no solution.

The formulation in Eq.~\eqref{eq:classification1} is however not satisfactory.
Why? Because not every point $\vx$ in the input space $\mathbb{R}^d$ is born
equal. Let us consider the previous example of a video-based intrusion detection
system again. Because the camera will be installed in a fixed location pointing toward
the store front, video frames will generally look similar to each other, and
will only form a very small subset of all possible video frames, unless some
exotic event happens. In this case, we would only care whether our alarm $M$
works well for those frames showing the store front and people entering or
leaving the store. Whether the distance between the reference machine and my
alarm is small for a video frame showing the earth from the outer space would
not matter at all.

And, here comes probability. We will denote by $p_X(\vx)$ the probability
(density) assigned to the input $\vx$ under the distribution $X$, and assume
that this probability reflects how likely the input $\vx$ is. We want to
emphasize the impact of the distance $D$ on likely inputs (high $p_X(\vx)$)
while ignoring the impact on unlikely inputs (low $p_X(\vx)$). In other words,
we weight each per-example cost with the probability of the corresponding
example. Then the problem of supervised learning becomes 
\begin{align}
    \label{eq:expected_loss0}
    \argmin_{M\in H} \int_{\mathbb{R}^d} p_X(\vx) D(M^*(\vx), M, \vx) \text{d}\vx 
    = \argmin_{M \in H} \mathbb{E}_{\vx \sim X} \left[ D(M^*(\vx),  M, \vx)
    \right].
\end{align}

Are we done yet? No, we still need to consider one more hidden cost in order to
make the description of supervised learning more complete. This hidden cost
comes from the operational cost, or {\it complexity}, of each machine $M$ in the
hypothesis set $H$.  It is reasonable to think that some machines are cheaper or
more desirable to use than some others are. Let us denote this cost of a machine
by $C(M)$, where $C: H \to \mathbb{R}_+$. Our goal is now slightly more
complicated in that we want to find a machine that minimizes both the cost in
Eq.~\eqref{eq:expected_loss0} and its operational cost. So, at the end, we get
\begin{align}
    \label{eq:expected_loss1}
    \argmin_{M \in H} \underbrace{\mathbb{E}_{\vx \sim X} \left[ D(M^*(\vx), M, \vx) \right]
        + \lambda C(M)
    }_{
        \mathclap{\text{Expected Cost}}
    },
\end{align}
where $\lambda \in \mathbb{R}_+$ is a coefficient that trades off the importance
between the expected distance (between $M^*$ and $M$) and the operational cost
of $M$.

In summary, supervised learning is a problem of finding a machine $M$ such that
has bot the low expectation of the distance between the outputs of $M^*$ and
$M$ over the input distribution and the low operational cost.

\paragraph{In Reality}

It is unfortunately impossible to solve the minimization problem in
Eq.~\eqref{eq:expected_loss1} in reality. There are so many reasons behind this,
but the most important reason is the input distribution $p_X$ or lack thereof.
We can decide on a distance function $D$ ourselves based on our goal. We can
decide ourselves a hypothesis set $H$ ourselves based on our requirements and
constraints. All good, but $p_X$ is not controllable in general, as it reflects
how the world is, and the world does not care about our own requirements nor
constraints.  

Let's take the previous example of video-based intrusion system. Our reference
machine $M^*$ is a security expert who looks at a video frame (and a person
within it) and determines whether that person is an intruder. We may decide to
search over any arbitrary set of neural networks to minimize the expected loss.
We have however absolutely no idea what the precise probability $p(\vx)$ of any
video frame. Instead, we only observe $\vx$'s which was randomly sampled from
the input distribution by the surrounding environment. We have no access to the
input distribution itself, but what comes out of it. 

We only get to observe a {\it finite} number of such samples $\vx$'s, with which
we must approximate the expected cost in Eq.~\eqref{eq:expected_loss1}. This
approximation method, that is approximation based on a finite set of samples
from a probability distribution, is called a {\it Monte Carlo method}.  Let us
assume that we have observed $N$ such samples: $\left\{ \vx^1, \ldots, \vx^N
\right\}$. Then we can approximate the expected cost by
\begin{align}
    \label{eq:empirical_loss}
    \underbrace{\mathbb{E}_{\vx \sim X} \left[ D(M^*(\vx), M, \vx) \right] +
    \lambda C(M)}_{
        \mathclap{\text{Expected Cost}}
    } =
    \underbrace{
        \frac{1}{N} \sum_{n=1}^N D(M^*(\vx^n), M, \vx^n)
        + \lambda C(M)
    }_{\mathclap{\text{Empirical Cost}}} + \epsilon,
\end{align}
where $\epsilon$ is an approximation error. We will call this cost, computed
using a finite set of input vectors, an {\it empirical cost}. 

\paragraph{Inference}

We have so far talked about what is a correct way to find a machine $M$ for our
purpose. We concluded that we want to find $M$ by minimizing the empirical cost
in Eq.~\eqref{eq:empirical_loss}. This is a good start, but let's discuss why we
want to do this first. There may be many reasons, but often a major complication
is the expense of running the reference machine $M^*$ or the limited access to
the reference machine $M^*$. Let us hence make it more realistic by assuming
that we will have access to $M^*$ only once at the very beginning together with
a set of input examples. In other words, we are given
\begin{align*}
    D_{\text{tra}} = \left\{ (\vx^1, M^*(\vx^1)), \ldots, (\vx^N,
    M^*(\vx^N))\right\},
\end{align*}
to which we refer as a {\it training set}. Once this set is available, we can
find $M$ that minimizes the empirical cost from Eq.~\eqref{eq:empirical_loss}
without ever having to query the reference machine $M^*$. 

Now let us think of what we would do when there is a {\it new} input $\vx \notin
D_{\text{tra}}$. The most obvious thing is to use $\hat{M}$ that minimizes the
empirical cost, i.e., $\hat{M}(\vx)$. Is there any other way? Another way is to
use all the models in the hypothesis set, instead of using only one model.
Obviously, not all models were born equal, and we cannot give all of them the
same chance in making a decision. Preferably we give a higher weight to the
machine that has a lower empirical cost, and also we want the weights to sum to
1 so that they reflect a properly normalized proportion. Thus, let us
(arbitrarily) define, as an example, the weight of each model as:
\begin{align*}
    \omega(M) = \frac{1}{Z} \exp\left( -J(M, D_{\text{tra}} ) \right),
\end{align*}
where $J$ corresponds to the empirical cost, and 
\begin{align*}
    Z = \sum_{M \in H} \exp\left( -J(M, D_{\text{tra}} \right)
\end{align*}
is a normalization constant. 

With all the models and their corresponding weights, I can now think of many
strategies to {\it infer} what the output of $M^*$ given the new input $\vx$.
Indeed, the first approach we just talked about corresponds to simply taking the
output of the model that has the highest weight. Perhaps, I can take the
weighted average of the outputs of all the machines:
\begin{align*}
    \sum_{M \in H} \omega(M) M(\vx),
\end{align*}
which is equivalent to $\mathbb{E}\left[ M(\vx) \right]$ under our arbitrary
construction of the weights.\footnote{
    {\it Is it really arbitrary, though?}
} We can similarly check the variance of the prediction. Perhaps I want to
inspect a set of outputs from the top-$K$ machines according to the weights.

We will mainly focus on the former approach, which is often called {\it maximum
a posteriori} (MAP), in this course. However, in a few of the lectures, we will
also consider the latter approach in the framework of {\it Bayesian} modelling.


\section{Perceptron}
\label{sec:perceptron}

Let us examine how this concept of supervised learning is used in practice by
considering a binary classification task. Binary classification is a task in
which an input vector $\vx \in \mathbb{R}^d$ is classified into one of two
classes, negative (0) and positive (1). In other words, a machine $M$ takes as
input a $d$-dimensional vector and outputs one of two values. 

\paragraph{Hypothesis Set}

In perceptron, a hypothesis set is defined as
\begin{align*}
    H = \left\{ 
    M | M(\vx) = \sign(\vw^\top \tilde{\vx}), \vw \in \mathbb{R}^{d+1}
    \right\},
\end{align*}
where $\tilde{\vx} = \left[ \vx; 1\right]$ denotes concatenating $1$ at the end
of the input vector $\vx$,\footnote{
    Why do we augment the original input vector $\vx$ with an extra $1$? What is
    an example in which this extra $1$ is necessary?  This is left to you as a
    {\bf homework assignment}.
}
and 
\begin{align}
    \label{eq:sign}
    \sign(x) = \left\{ \begin{array}{l l}
            0,&\text{ if } x \leq 0, \\
            1,&\text{ otherwise}
        \end{array}
        \right..
\end{align}
In this section, we simply assume that each and every machine in this hypothesis
set has a constant operational cost $c$, i.e., $C(M)=c$ for all $M\in H$. 

\paragraph{Distance}

\todo{rewrite!}

Given an input $\vx$, we now define a distance between $M^*$ and $M$. In
particular, we will use the following distance function:\footnote{
    There is a problem with this distance function. What is it?  This is left to
    you as a {\bf homework assignment}.
}
\begin{align}
    \label{eq:perceptron_dist}
    D(M^*(\vx), M, \vx) = -\underbrace{\left( M^*(\vx) - M(\vx)
    \right)}_{\text{(a)}} \underbrace{\left(\vw^\top
    \tilde{\vx}\right)}_{\text{(b)}}.
\end{align}
The term (a) states that the distance between the predictions of the reference
and our machines is $0$ as long as their predictions coincide. When it is not,
the term (a) will be 1 if $M^*(\vx) = 1$ and -1 if $M^*(\vx) = 0$.

When it is not, the term (a) will be 1, which is when the term (b) comes into
play. The dot product in (b) computes how well the weight vector $\vw$ and the
input vector $\vx$ aligns with each other.\footnote{
    $\vw^\top \tilde{\vx} = \sum_{i=1}^{d+1} w_i x_i$.
} When they are positively aligned (pointing in the same direction), this term
will be positive, making the output of the machine $1$. When they are negative
aligned (pointing in opposite directions), it will be negative with the output
of the machine $M$ $0$.

Considering both (a) and (b), we see that the smallest value $D$ can take is $0$
,when the prediction is correct,\footnote{
    There is one more case. What is it?
} and otherwise, positive. When the term (a) is 1, $\vw^\top \tilde{\vx}$ is
negative, because $M(\vx)$ was 0, and the overall distance becomes positive
(note the negative sign at the front.) When the term (b) is -1, $\vw^\top
\tilde{\vx}$ is positive, because $M(\vx)$ was 1, in which case the distance is
again positive. 

What should we do in order to decrease this distance, if it is non-zero? We want
to make the weight vector $\vw$ to be aligned more positively with $M^*(\vx)$,
if the term (a) is 1, which can be done by moving $\vw$ toward $\tilde{\vx}$. In
other words, the distance $D$ shrinks if we add a bit of $\tilde{\vx}$ to $\vw$,
i.e., $\vw \leftarrow \vw + \eta \tilde{\vw}$. If the term (a) is -1, we should
instead push $\vw$ so that it will {\it negatively} align with $\tilde{\vx}$,
i.e., $\vw \leftarrow \vw - \eta \tilde{\vw}$. These two cases can be unified by
\begin{align}
    \label{eq:perceptron_rule0}
    \vw \leftarrow \vw + \eta \left( M^*(\vx) - M(\vx)\right) \tilde{\vx},
\end{align}
where $\eta$ is often called a {\it step size} or {\it learning rate}.  We can
repeat this update until the term (a) in Eq.~\eqref{eq:perceptron_dist} becomes
0. 

\paragraph{Learning}

As discussed earlier, we assume that we make only a finite number of queries to
the reference machine $M^*$ using a set of inputs drawn from the unknown input
distribution $p(\vx)$. This results in our training dataset:
\begin{align*}
    D_{\text{tra}} = \left\{ (\vx_1, y_1), \ldots, (\vx_N, y_N) \right\},
\end{align*}
where we begin to use a short hand $y_n=M^*(\vx_n)$.

With this dataset, our goal now is to find $M \in H$ that has the least
empirical cost in Eq.~\eqref{eq:empirical_loss} with the distance function
defined in Eq.~\eqref{eq:perceptron_dist}. Combining these two, we get
\begin{align}
    \label{eq:perceptron_cost}
    J(\vw, D_{\text{tra}}) = -\frac{1}{N} \sum_{n=1}^N 
    \left( y_n - M(\vx_n) \right) 
    \left(\vw^\top \tilde{\vx_n}\right).
\end{align}

We will again resort to an iterative method for minimizing this empirical cost
function, as we have done with a single input vector above. What we will do is
to collect all those input vectors on which $M$ (or equivalently $\vw$) has made
mistakes. This is equivalent to considering only those input vectors where
$y_n-M(\vx_n) \neq 0$. Then, we collect all the {\it update directions},
computed using Eq.~\eqref{eq:perceptron_rule0}, and move the weight vector
$\vw$ toward its average. That is,
\begin{align}
    \label{eq:perceptron_rule1}
    \vw \leftarrow \vw + \eta \frac{1}{N} \sum_{n=1}^N \left( y_n - M(\vx_n)\right) \tilde{\vx}.
\end{align}
We apply this rule repeatedly until the empirical cost in
Eq.~\eqref{eq:perceptron_cost} does not improve (i.e., decrease). 

This learning rule is known as a {\it perceptron learning rule}, which was
proposed by Rosenblatt in 1950's \cite{Rosenblatt1962}, and has a nice property
that it will find a correct $M$ in the sense that the empirical cost is at its
minimum (0), {\it if} such $M$ is in $H$. In other words, if our hypothesis set
$H$ is good and includes a reference machine $M^*$, this perceptron learning
rule will eventually find an equally good machine $M$. It is important to note
that there may be many such $M$, and the perceptron learning rule will find one
of them. 


\section{Logistic Regression}
\label{sec:logreg}

The perceptron is not entirely satisfactory for a number of reasons. One
of them is that it does not provide a well-calibrated measure of the degree to
which a given input is either negative or positive. That is, we want to know not
whether it is negative or positive but rather how likely it is negative. It is
then natural to build a machine that will output the probability $p(C|\vx)$,
where $C \in \left\{ -1, 1\right\}$.

\paragraph{Hypothesis Set} 

To do so, let us first modify the definition of a machine $M$. $M$ now takes as
input a vector $\vx \in \mathbb{R}^d$ and returns a probability $p(C|\vx) \in
\left[ 0, 1\right]$ rather than $\left\{ 0, 1\right\}$. We only need to change
just one thing from the perceptron, that is
\begin{align}
    \label{eq:logreg}
    M(\vx) = \sigmoid(\vw^\top \tilde{\vx}),
\end{align}
where $\sigmoid$ is a sigmoid function defined as
\begin{align*}
    \sigmoid(a) = \frac{1}{1 + \exp(-a)}
\end{align*}
and is bounded by $0$ from below and by $1$ from above. Suddenly this machine
does not return the prediction, but the probability of the prediction being
positive (1). That is,
\begin{align*}
    p(C=1|\vx) = M(\vx).
\end{align*}
Naturally, our hypothesis set is now
\begin{align*}
    H = \left\{ 
    M | M(\vx) = \sigmoid(\vw^\top \tilde{\vx}), \vw \in \mathbb{R}^{d+1}
    \right\},
\end{align*}
where $\tilde{\vx} = \left[ \vx; 1\right]$ denotes concatenating $1$ at the end
of the input vector as before. 

\paragraph{Distance}

The distance is not trivial to define in this case, because the things we want
to measure the distance between are not directly comparable. One is an element
in a discrete set (0 or 1), and the other is a probability. It is helpful now to
think instead about {\it how often} a machine $M$ will agree with the reference
machine $M^*$, if we randomly sample the prediction given its output
distribution $p(C|\vx)$. This is exactly equivalent to $p(C=M^*(\vx)|\vx)$. In
this sense, the distance between the reference machine $M^*$ and our machine $M$
given an input vector $\vx$ is smaller than this frequency of $M$ being correct
is larger, and vice versa. Therefore, we define the distance as the negative
log-probability of the $M$'s output being correct:
\begin{align}
    \label{eq:logreg_dist}
    D(M^*(\vx), M, \vx) =& -\log p(C=M^*(\vx) | \vx) \\
    =& -(M^*(\vx) \log M(\vx) + (1-M^*(\vx)) \log (1- M(\vx))),
    \nonumber
\end{align}
where $p(C=1|\vx) = M(\vx)$. The latter equality comes from the definition of
{\it Bernoulli distribution}.\footnote{
    \todo{give a brief def of Bernoulli distribution}
}

With this definition of our distance, how do we adjust $\vw$ of $M$ to lower it?
In the case of perceptron, we were able to manually come up with an {\it
algorithm} by looking at the perceptron distance in
Eq.~\eqref{eq:perceptron_dist}. It is however not too trivial with this logistic
regression distance.\footnote{
    It may be trivial to some who have great mathematical intuition.
}
Thus, we now turn to Calculus, and use the fact that the {\it gradient} of a
function points to the direction toward which its output increases (at least
locally). 

The gradient of the above logistic regression distance function with respect to
the weight vector $\vw$ is\footnote{
    The step-by-step derivation of this is left to you as a {\bf homework
    assignment}. 
}
\begin{align}
    \label{eq:grad_logreg_dist}
    \nabla_{\vw} D(M^*(\vx), M, \vx) = -(M^*(\vx) - M(\vx)) \tilde{\vx}.
\end{align}
When we move the weight vector ever so slightly in the opposite direction, the
logistic regression distance in Eq.~\eqref{eq:logreg_dist} will decrease. That
is,
\begin{align}
    \label{eq:logreg_rule0}
    \vw \leftarrow \vw + \eta \left( M^*(\vx) - M(\vx)\right) \tilde{\vx},
\end{align}
where $\eta \in \mathbb{R}$ is a small scalar and called either a {\it step
size} or {\it learning rate}. 

\paragraph{Coincidence?}

Is it surprising to realize that this rule for logistic regression is identical
to the perceptron rule in Eq.~\eqref{eq:perceptron_rule0}? Let us see what this
logistic regression rule, or equivalent to perceptron learning rule, does by
focusing on the update term (the second term in the learning rule). The first
multiplicative factor $(M^*(\vx) - M(\vx))$ can be written as
\begin{align}
    \label{eq:grad_logreg_term1}
    M^*(\vx) - M(\vx) = \underbrace{\overline{\sign}(M^*(\vx) - M(\vx))}_{\text{(a)}}
    \underbrace{\left| M^*(\vx) - M(\vx) \right|}_{\text{(b)}},
\end{align}
where 
\begin{align*}
    \overline{\sign}(x) = \left\{ \begin{array}{l l}
            -1,&\text{ if } x \leq 0, \\
            1,&\text{ otherwise}
        \end{array}
        \right.
\end{align*}
is a symmetric sign function (compare it to the asymmetric sign function in
Eq.~\eqref{eq:sign}.)

The term (a) effectively tell us {\it in which way} the machine is wrong about
the input $\vx$. Is $M$ saying it is likely to be $0$ when the reference machine
says $1$, or is $M$ saying it is likely to be $1$ when the reference machine
says $0$? In the former case, we want the weight vector $\vw$ to align more with
$\vx$, and thus the positive sign. In the latter case, we want the opposite, and
hence the negative sign.  The second term (b) tells us {\it how much} the
machine is wrong about the input $\vx$. This term ignores in which direction the
machine is wrong, since it is computed by the term (a), but entirely focuses on
how {\it far} the model's prediction is from that of the reference machine. If
the prediction is close to that of the reference machine, we only want the
weight vector to move ever so slowly.

The second term in both the logistic regression and perceptron rules is the
input vector, augmented with an extra $1$. This term is there, because the
prediction by a machine $M$ is made based on how well the weight vector and the
input vector align with each other, which is computed as a dot product between
these two vectors. 

In other words, it is not a coincidence, but only natural that they are
equivalent, as both of them effectively solve the same problem of binary
classification. In the case of perceptron, we have reached its learning rule
based on our observation of the process, while in the case of logistic
regression, we relied on a more mechanical process of using gradient to find the
steepest ascent direction. The latter approach is often more desirable, as it
allows us to apply the same procedure (update the machine following the steepest
descent direction,) however with a constraint that the empirical cost be
differentiable (almost everywhere\footnote{
    We will see why the cost function does not have to be differentiable
    everywhere later in the course.
}) with respect to the machine's parameters. We will thus largely stick to this
kind of gradient-based optimization to minimize any distance function from here
on.

The only major difference between the learning rules for the logistic regression
and perceptron is whether the term (b) in Eq.~\eqref{eq:grad_logreg_term1} is
discrete (in the case of perceptron) or continuous (in the case of logistic
regression.) More specifically, the term (b) in the perceptron learning rule is
either $0$ or $1$.  \todo{}

\paragraph{Learning}

With the distance function defined, we can write a full empirical cost as
\begin{align*}
    J(\vw, D_{\text{tra}}) = -\frac{1}{N} \sum_{n=1}^N 
    y_n \log M(\vx_n) + (1-y_n) \log (1- M(\vx_n)).
\end{align*}
We assume that the operational cost, or complexity, of each $M$ can be ignored. 
Similarly to what we have done for minimizing (decreasing) the distance between
$M$ and $M^*$ given a single input vector, we will use the gradient of the whole
empirical cost function to decide how we change the weight vector $\vw$. The
gradient is 
\begin{align*}
    \nabla_{\vw} J = -\frac{1}{N} \sum_{n=1}^N \nabla_{\vw} D(y_n, M, \vx_n),
\end{align*}
which is simply the average of the gradients of the distances from
Eq.~\eqref{eq:grad_logreg_dist} over the whole training set.

The fact that the empirical cost function is (twice) differentiable with respect
to the weight vector allows us to use any advanced gradient-based optimization
algorithm. \todo{}

\todo{}

\section{Overfitting, Regularization and Complexity}

\subsection{Overfitting: Generalization Error}

\todo{}

\subsection{How Wrong is Wrong?}

\todo{}

\section{Linear Separability and Nonlinear Classifiers}

\subsection{Classification as Scoring}

\paragraph{Decision Boundary}

Both the perceptron from Sec.~\ref{sec:perceptron} and logistic regression from
Sec.~\ref{sec:logreg} define a {\it hyperplane} based on which the decision is
made. One side of the hyperplane is classified as positive (1), and the other
side negative (0). This hyperplane is characterized as
\begin{align}
    \label{eq:lin_hyperplane}
    \vw^\top \tilde{\vx} = 0,
\end{align}
where $\tilde{\vx}=\left[ \vx; 1\right]$.  Because this hyperplane (or a line in
the case of 2-D space) corresponds to the boundary between two decisions, it is
often called a {\it decision boundary}. In other words, the decision boundary of
the perceptron or logistic regression is {\it linear}. 

\paragraph{A Classifier as a Score Function}

Let us define a score function $s$ as a dot product between the weight vector
$\vw$ and an input vector $\vx$, that is $s(\vx; \vw) = \vw^\top \tilde{\vx}$. The
machine that defines a perceptron is than $\sign(s(\vx; \vw))$, and that for
logistic regression is $\sigmoid(s(\vx; \vw))$. In either of these, the final
decision (0 or 1) is made based on whether the score of an input vector $\vx$ is
negative or positive. When the score is $0$, the input vector falls right on the
decision boundary (or the hyperplane in Eq.~\eqref{eq:lin_hyperplane}.) In other
words, the decision boundary is defined as a set of points $\vx$ in the input
space of which their scores are $0$. That is,
\begin{align*}
    s(\vx; \vw) = 0.
\end{align*}
When the score function\footnote{
    Note that the term ``score'' has a number of different meanings. For
    instance, in statistics, the score is defined as a gradient of the
    \todo{TODO}
}
is defined as a linear function of the input vector
$\vx$, we call it a {\it linear classifier}, and otherwise, a {\it nonlinear
classifier}. 

\todo{}


\section{$k$-Nearest Neighbours and Radial Basis Function Networks}

\subsection{Radial Basis Function Networks}

Let us consider the inference method in kernel support vector machines from the
previous section. 
\begin{align*}
    M(\vx) = \sign \left(\sum_{j=1}^J \alpha_j k(\vx_j, \vx)\right).
\end{align*}


\subsection{$k$-Nearest Neighbours}


\section{Support Vector Machines: Nonlinear Feature Functions and a Kernel Trick}

\subsection{One Problem, Many Cost Functions}

\subsection{Support Vector Machines}

\subsection{Kernel Methods}


\section{Decision Tree$^\star$}

\section{Ensemble Methods$^\star$}


\chapter{Regression}
\label{sec:regression}

We have so far considered a problem of classification, where the output of a
machine $M$ is constrained to be a finite set of discrete labels/classes. In
this section, we consider a {\it regression} problem in which case the machine
outputs an element from an infinite set.

\section{Linear Regression}

\subsection{Linear Regression}


\subsection{Regularization and Prior Distributions}


\section{Bayesian Linear Regression and Gaussian Process Regression}


\subsection{Bayesian Approach to Machine Learning}


\subsection{Gaussian Process Regression$^\star$}


\chapter{Dimensionality Reduction}
\label{chap:dimred}

\section{Unsupervised Learning: Problem Setup}

Unsupervised learning is a weird, but fascinating problem. Unlike supervised
learning in which a machine was defined as a transformation of an input vector
$\vx$, a machine $M$ in unsupervised learning {\it generates} an input vector.

\subsection{Naive Bayes Classifier}

\todo{}

\section{Dimensionality Reduction: Problem Setup}
\label{sec:dimred}

\section{Principal Component Anslysis}

\subsection{Maximum Variance Criterion}

\subsection{Minimum Reconstruction Criterion}

%\subsection{PCA with Missing Values$^\star$: Collaborative Filtering}

\subsection{Probabilistic Principal Component Analysis}

\paragraph{Expectation-Maximization Algorithm}

\section{Other Dimensionality Reduction Techniques$^\star$}


\chapter{Clustering}
\label{sec:cluster}

\section{Problem Setup}

\subsection{Clustering vs. Dimensionality Reduction}

\section{$k$-Means Clustering}

\section{Mixture of Gaussians$^\star$}

\section{Other Clustering Methods$^\star$}


\chapter{Structured Output Prediction and Reinforcement Learning}


\section{Time-Series Modelling$^\star$}
\label{sec:timeseries}









\bibliographystyle{abbrv}
\bibliography{lecture_note}


\end{document}






